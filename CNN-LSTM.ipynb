{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamflow_data = pd.read_csv(\"data/streamflow_data/Final_Processed_Station_Data_Watershed.csv\")\n",
    "streamflow_data.dropna(inplace=True)\n",
    "streamflow_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_lats = streamflow_data.iloc[0][1:]\n",
    "station_lons = streamflow_data.iloc[1][1:]\n",
    "watersheds = streamflow_data.iloc[2][1:]\n",
    "\n",
    "# drop the first 3 rows\n",
    "streamflow_data = streamflow_data.drop([0, 1, 2])\n",
    "# drop the first column\n",
    "streamflow_data = streamflow_data.drop(columns=[\"name\"])\n",
    "streamflow_data = streamflow_data.astype(float)\n",
    "streamflow_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the precipitation and temperature data\n",
    "rainfall_data = np.load(\"data/weather_data/rainfall_data.npy\").astype(float)\n",
    "snowfall_data = np.load(\"data/weather_data/snowfall_data.npy\").astype(float)\n",
    "max_temp_data = np.load(\"data/weather_data/max_temp_data_interp.npy\").astype(float)\n",
    "min_temp_data = np.load(\"data/weather_data/min_temp_data_interp.npy\").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the fractions\n",
    "train_frac = 7/9\n",
    "val_frac = 1/9\n",
    "test_frac = 1/9\n",
    "\n",
    "assert train_frac + val_frac + test_frac == 1\n",
    "\n",
    "# define the indices for the train, validation, and test sets\n",
    "train_idx = int(train_frac * len(streamflow_data))\n",
    "val_idx = int((train_frac + val_frac) * len(streamflow_data))\n",
    "n_train = train_idx\n",
    "n_val = val_idx - train_idx\n",
    "n_test = len(streamflow_data) - val_idx\n",
    "\n",
    "assert n_train + n_val + n_test == len(streamflow_data)\n",
    "\n",
    "# standardize the data based on the training set\n",
    "rainfall_data = (rainfall_data - np.mean(rainfall_data[:train_idx])) / np.std(rainfall_data[:train_idx])\n",
    "snowfall_data = (snowfall_data - np.mean(snowfall_data[:train_idx])) / np.std(snowfall_data[:train_idx])\n",
    "max_temp_data = (max_temp_data - np.mean(max_temp_data[:train_idx])) / np.std(max_temp_data[:train_idx])\n",
    "min_temp_data = (min_temp_data - np.mean(min_temp_data[:train_idx])) / np.std(min_temp_data[:train_idx])\n",
    "streamflow_data = (streamflow_data - streamflow_data.iloc[:train_idx].mean()) / streamflow_data.iloc[:train_idx].std()\n",
    "streamflow_data = streamflow_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the time window\n",
    "n_groups = 26\n",
    "group_size = 7\n",
    "time_window = n_groups * group_size\n",
    "n_channels = 4\n",
    "grid_shape = rainfall_data.shape[1:]\n",
    "n_stations = streamflow_data.shape[1]\n",
    "\n",
    "combine_multiple_groups = False\n",
    "if combine_multiple_groups:\n",
    "    n_groups = 28\n",
    "    pts_per_group = 7\n",
    "    group_sizes = [1,7,14,30]\n",
    "    time_window = np.sum(group_sizes) * pts_per_group\n",
    "\n",
    "# create the training, validation, and test sets\n",
    "x_intermediate = np.empty(np.shape(rainfall_data) + (n_channels,), dtype='single')\n",
    "for i, data in enumerate([rainfall_data, snowfall_data, max_temp_data, min_temp_data]):\n",
    "    x_intermediate[:,:,:,i] = data\n",
    "\n",
    "def gen_train():\n",
    "    for i in range(n_train - time_window):\n",
    "        xx = tf.convert_to_tensor(x_intermediate[i:i+time_window].reshape(n_groups, *grid_shape, n_channels, group_size).mean(axis=-1))\n",
    "        yy = streamflow_data[time_window + i,:]\n",
    "        yield (xx, yy)\n",
    "\n",
    "def gen_val():\n",
    "    for i in range(n_val):\n",
    "        xx = tf.convert_to_tensor(x_intermediate[i+n_train-time_window:i+n_train].reshape(n_groups, *grid_shape, n_channels, group_size).mean(axis=-1))\n",
    "        yy = streamflow_data[val_idx + i,:]\n",
    "        yield (xx, yy)\n",
    "\n",
    "def gen_test():\n",
    "    for i in range(n_test):\n",
    "        xx = tf.convert_to_tensor(x_intermediate[i+val_idx-time_window:i+val_idx].reshape(n_groups, *grid_shape, n_channels, group_size).mean(axis=-1))\n",
    "        yy = streamflow_data[val_idx + i,:]\n",
    "        yield (xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Input layer\n",
    "    cnn_input = layers.Input(shape=(n_groups, 22, 37, n_channels), name=\"Weather_Data_Input\")\n",
    "    \n",
    "    # TimeDistributed CNN\n",
    "    cnn = layers.TimeDistributed(layers.Conv2D(8, (3, 3), activation='relu', padding='same'))(cnn_input)\n",
    "    cnn = layers.TimeDistributed(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))(cnn)\n",
    "    cnn = layers.TimeDistributed(layers.MaxPooling2D((2, 2)))(cnn)\n",
    "    cnn = layers.TimeDistributed(layers.Flatten())(cnn)  # Flatten the grid\n",
    "\n",
    "    # LSTM for temporal patterns\n",
    "    lstm = layers.LSTM(64, return_sequences=False, activation='tanh')(cnn)\n",
    "\n",
    "    # Fully connected layers\n",
    "    dense = layers.Dense(32, activation='relu')(lstm)\n",
    "    dense = layers.Dropout(0.2)(dense)  # Dropout for regularization\n",
    "    output = layers.Dense(n_stations, activation='linear', name=\"Streamflow_Output\")(dense)\n",
    "\n",
    "    # Model definition\n",
    "    model = models.Model(inputs=cnn_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an ensemble of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Best model not yet determined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model not yet determined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Best model not yet determined"
     ]
    }
   ],
   "source": [
    "raise NotImplementedError(\"Best model not yet determined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets and train an ensemble\n",
    "batch_size = 8\n",
    "ensemble_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             mode='min',\n",
    "                                             verbose=1,\n",
    "                                             patience=5,\n",
    "                                             restore_best_weights=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.1,\n",
    "                                                patience=2,\n",
    "                                                verbose=1,\n",
    "                                                mode='min')\n",
    "\n",
    "for i in range(ensemble_size):\n",
    "    print(f\"Training model {i+1}/{ensemble_size}\")\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        gen_train,\n",
    "        (tf.float16, tf.float16),\n",
    "        (tf.TensorShape([time_window, *grid_shape, n_channels]), tf.TensorShape([n_stations]))\n",
    "    ).shuffle(n_train).batch(batch_size)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        gen_val,\n",
    "        (tf.float16, tf.float16),\n",
    "        (tf.TensorShape([time_window, *grid_shape, n_channels]), tf.TensorShape([n_stations]))\n",
    "    ).shuffle(n_val).batch(batch_size)\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=40,\n",
    "        validation_data=val_dataset,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "    )\n",
    "\n",
    "    # save the model\n",
    "    model.save(f\"models/ensemble_model_{i}.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the ensemble\n",
    "ensemble = []\n",
    "for i in range(4):\n",
    "    model = tf.keras.models.load_model(f\"models/ensemble_model_{i}.keras\")\n",
    "    ensemble.append(model)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    gen_val,\n",
    "    (tf.float16, tf.float16),\n",
    "    (tf.TensorShape([time_window, *grid_shape, n_channels]), tf.TensorShape([n_stations]))\n",
    ").batch(batch_size)\n",
    "\n",
    "ensemble_predictions = []\n",
    "for model in ensemble:\n",
    "    ensemble_predictions.append(model.predict(val_dataset))\n",
    "\n",
    "ensemble_mean = np.mean(ensemble_predictions, axis=0)\n",
    "\n",
    "# calculate the RMSE\n",
    "val_data = np.zeros((n_val, n_stations))\n",
    "for i, (xx, yy) in enumerate(gen_val()):\n",
    "    val_data[i] = yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model predictions for the first 20 stations using subplots\n",
    "n_plot = 20\n",
    "\n",
    "predictions = ensemble_mean\n",
    "\n",
    "fig, axs = plt.subplots(n_plot // 2, 2, figsize=(16, n_plot * 2))\n",
    "for i in range(n_plot):\n",
    "    mse = np.mean((val_data[:, i] - predictions[:, i]) ** 2)\n",
    "    station = i\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    ax.plot(np.array(ensemble_predictions).T[i], c='k', alpha=0.2)\n",
    "    ax.plot(val_data[:, station], label='Actual')\n",
    "    ax.plot(predictions[:, station], label='Predicted')\n",
    "    ax.set_title(f\"Station {station} (MSE: {mse:.3f})\")\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
